{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5e78268",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The key terms for today\n",
    "\n",
    "* supervised\n",
    "* unsupervised\n",
    "* classification\n",
    "* clustering\n",
    "* reinforcement learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "482c7d82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's talk about machine learning\n",
    "\n",
    "The phrase \"machine learning\" refers to any method for approximating a solution to a problem for which we don't have an analytical solution (an algorithmic solution) through examining data. The basic taxonomy of machine learning approaches is depicted below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![ML algorithms](https://blogs.sas.com/content/subconsciousmusings/files/2017/04/machine-learning-cheet-sheet-2.png)\n",
    "\n",
    "*Image from https://blogs.sas.com/content/subconsciousmusings/*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, this diagram does not include a third major class of ML algorithm, reinforcement learning, which has been used (among many other applications!) to develop ChatGPT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f4a8497",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we talk about machine learning, we talk about:\n",
    "* *fitting* (or *training*) a *prediction function*, or *model*, to\n",
    "* *training* data, experimenting with various\n",
    "* *hyperparameters* related to the *model architecture* using held-out\n",
    "* *development* data, so that the resulting model generalizes well, making good *predictions* on held-out\n",
    "* *test* (or *evaluation*) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The goal of unsupervised learning is to uncover latent structure or patterns in the data. An example of an application of unsupervised learning is topic modeling. \n",
    "\n",
    "The goal of supervised learning is to learn to match the labels (or answers, or ground truth, or dependent variable) in the data. An example of an application of supervised learning is part of speech tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to investigate ML further, here is a great python library for ML:\n",
    "* scikit-learn (sklearn): https://scikit-learn.org/stable/index.html\n",
    "\n",
    "Sklearn uses a pattern; for each ML algorithm, there is a *fit* function (for training), a *predict* function (for inference or testing), and a *score* function (for evaluation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We evaluate models / prediction functions using [any number of metrics](https://scikit-learn.org/stable/modules/model_evaluation.html). A commonly used one for supervised machine learning is:\n",
    "* accuracy - what percent of the data points were classified correctly?\n",
    "\n",
    "Of course, accuracy is just one number. To get a clearer understanding, we can construct a\n",
    "* confusion matrix\n",
    "\n",
    "which has the classes (the labels) along rows and columns, and in each cell indicates the number of data points classified as *row* that are truly in class *column*. \n",
    "\n",
    "We will look more at confusion matrices later this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question!\n",
    "\n",
    "If the transformer is the model architecture, and sentiment analysis is the task:\n",
    "* What is some training data we could use?\n",
    "* What are some hyperparameters we could set?\n",
    "* What is a metric we could use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Transformers\n",
    "\n",
    "With transformers, we talk about three types or stages of \"training\" (only two of them involve training):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "1. Pretraining using self-supervision - the training data is the web (for text generation), or lots of random images (for image generation), and the label is whatever we remove from each training data instance: a word in the middle, a word at the end, some pixels.... We train to get a good representation of text (or images) *in general*. For pretraining, we need millions to billions of data points, but fortunately we don't have to manually label them all!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "2. Finetuning using labeled data - the training data is some data for a task, which a human has labeled. For example, it could be text with sentiment labels; or images with object classes. We take a pretrained transformer encoder model (which already knows about text/images/etc *in general*) and we train just a last layer on top *for this task*. (This can be done with seq2seq / encoder-decoder models too, but usually it's done with encoder models.) For fine tuning, we need a few hundred to a few thousand data points per class (per label).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "3. Few shot - we take a small amount of data and prompt the pretrained model with it, ending with an example we want the model to actually classify. For example, we might say: \"Complete the list. The movie was great: positive. The film was terrible: negative. The movie was directed by Sofia Coppola: neutral. The movie was bad: \" and then let the pretrained model complete the input. The model is just a great big pattern matcher, so it may complete with something correct (like 'negative') or something incorrect (like 'positive') or something spurious (like 'on Tuesday'). Bigger models will work pretty well for lots of tasks, though, if properly prompted with a few examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
